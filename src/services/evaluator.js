/**
 * AI ì±„íŒ… í‰ê°€ ì„œë¹„ìŠ¤
 * Gemini, OpenAI, Claude APIë¥¼ ì´ìš©í•œ ì±„íŒ… í‰ê°€
 */

/**
 * ì±„íŒ… ë‚´ìš©ì„ ë£¨ë¸Œë¦­ ê¸°ë°˜ìœ¼ë¡œ í‰ê°€
 */
export async function evaluateChat({ chatContent, reflection, rubric, apiSettings }) {
    // Destructure models from settings, support legacy/fallback
    const { provider, apiKeys } = apiSettings
    const models = apiSettings.models || {}
    const evaluationRuns = apiSettings.evaluationRuns || 1 // K-run support

    // Determine the specific model to use for single-provider mode
    // If not found in `models`, fallback to legacy `model` field or default
    const currentModel = models[provider] || apiSettings.model

    // Get API key for the selected provider
    const apiKey = apiKeys?.[provider] || apiSettings.apiKey || ''

    // Validation: Check if model is valid
    if (!currentModel || currentModel === 'custom' || currentModel.trim() === '') {
        throw new Error(`'${provider}'ì— ëŒ€í•œ ëª¨ë¸ ì´ë¦„ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ê´€ë¦¬ì ì„¤ì •ì—ì„œ 'ì§ì ‘ ì…ë ¥'ì„ ì„ íƒí•œ í›„ ëª¨ë¸ëª…(ì˜ˆ: gemini-pro-vision, gpt-4-turbo)ì„ ì •í™•íˆ ì…ë ¥í•´ì£¼ì„¸ìš”.`)
    }

    console.log(`Evaluating with Provider: ${provider}, Model: ${currentModel}, Runs: ${evaluationRuns}`)

    // í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
    const prompt = buildEvaluationPrompt(chatContent, rubric, reflection)

    // K-run evaluation: run multiple times and synthesize
    if (evaluationRuns > 1) {
        return await evaluateWithKRuns(prompt, provider, currentModel, apiKey, apiKeys, apiSettings, rubric, evaluationRuns)
    }

    // Single run (default) with fallback
    // Single run (default) with fallback and retries
    let lastError = null
    const MAX_RETRIES = 2

    for (let attempt = 0; attempt <= MAX_RETRIES; attempt++) {
        try {
            if (attempt > 0) console.log(`Retry attempt ${attempt}...`)

            let response = await singleEvaluation(prompt, provider, currentModel, apiKey, apiKeys, apiSettings)

            // ğŸ” DEBUG: Log raw response to identify parsing issues
            console.log('ğŸ” [DEBUG] RAW AI RESPONSE:', response)

            if (!response || response.trim() === '') {
                throw new Error('AI returned empty response')
            }

            const parsed = parseEvaluationResponse(response, rubric)

            // ğŸ” DEBUG: Log parsed result
            console.log('ğŸ” [DEBUG] PARSED RESULT:', parsed)

            // Temporary debugging hook
            if (typeof window !== 'undefined') {
                window.debugLastEvaluation = { raw: response, parsed }
            }

            return parsed
        } catch (error) {
            console.warn(`Evaluation attempt ${attempt + 1} failed:`, error.message)
            lastError = error

            // Last attempt fallback to server proxy (only if client-side failed and proxy is available)
            if (attempt === MAX_RETRIES && !apiSettings.useServerSide) {
                console.warn('All retries failed, trying server proxy backup...')
                try {
                    const response = await callServerProxy({
                        prompt,
                        provider,
                        model: currentModel,
                        apiKeys: apiSettings.apiKeys,
                        models: apiSettings.models || {}
                    })
                    return parseEvaluationResponse(response, rubric)
                } catch (serverError) {
                    throw new Error(`í‰ê°€ ì‹¤íŒ¨ (ì¬ì‹œë„ ${MAX_RETRIES}íšŒ í¬í•¨): ${error.message} (Server fallback also failed: ${serverError.message})`)
                }
            }
        }
    }

    throw lastError
}

/**
 * K-run parallel evaluation
 */
async function evaluateWithKRuns(prompt, provider, currentModel, apiKey, apiKeys, apiSettings, rubric, runs) {
    console.log(`Starting ${runs}-run parallel evaluation...`)

    const promises = []
    for (let i = 0; i < runs; i++) {
        promises.push(
            singleEvaluation(prompt, provider, currentModel, apiKey, apiKeys, apiSettings)
                .then(response => parseEvaluationResponse(response, rubric))
                .catch(err => {
                    console.warn(`Run ${i + 1} failed:`, err.message)
                    return null // Mark failed runs as null
                })
        )
    }

    const results = await Promise.all(promises)
    const successfulResults = results.filter(r => r !== null)

    if (successfulResults.length === 0) {
        throw new Error('ëª¨ë“  í‰ê°€ ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.')
    }

    console.log(`${successfulResults.length}/${runs} evaluations succeeded`)

    // Synthesize results
    return synthesizeKRunResults(successfulResults)
}

/**
 * Single evaluation call (extracted for reuse)
 */
async function singleEvaluation(prompt, provider, currentModel, apiKey, apiKeys, apiSettings) {
    const models = apiSettings.models || {}
    const hasRequiredKeys = !!apiKey
    const useServerProxy = apiSettings.useServerSide || !hasRequiredKeys

    if (useServerProxy) {
        return await callServerProxy({
            prompt,
            provider,
            model: currentModel,
            apiKeys: apiSettings.apiKeys,
            models: models
        })
    } else {
        switch (provider) {
            case 'gemini':
                return await callGeminiAPI(prompt, apiKey, currentModel)
            case 'openai':
                return await callOpenAIAPI(prompt, apiKey, currentModel)
            case 'claude':
                return await callClaudeAPI(prompt, apiKey, currentModel)
            default:
                throw new Error('ì§€ì›í•˜ì§€ ì•ŠëŠ” AI ì œê³µì—…ì²´ì…ë‹ˆë‹¤.')
        }
    }
}

/**
 * Synthesize multiple K-run results into one
 */
function synthesizeKRunResults(results) {
    const n = results.length

    // Calculate average score
    const avgScore = Math.round(results.reduce((sum, r) => sum + r.totalScore, 0) / n)
    const scores = results.map(r => r.totalScore)
    const minScore = Math.min(...scores)
    const maxScore = Math.max(...scores)

    // Calculate grade based on average
    const grade = calculateGrade(avgScore)

    // Synthesize criteria scores (average each)
    const criteriaScoresMap = {}
    results.forEach(result => {
        result.criteriaScores?.forEach(cs => {
            if (!criteriaScoresMap[cs.name]) {
                criteriaScoresMap[cs.name] = {
                    ...cs,
                    scoreSum: 0,
                    count: 0,
                    allDetails: []
                }
            }
            criteriaScoresMap[cs.name].scoreSum += cs.score
            criteriaScoresMap[cs.name].count++
            if (cs.details) {
                criteriaScoresMap[cs.name].allDetails.push(cs.details)
            }
        })
    })

    const criteriaScores = Object.values(criteriaScoresMap).map(cs => {
        // Collect all details directly from criteria scores (fallback to details object if needed)
        const allStrengths = cs.allDetails.flatMap(d => d.strengths || d?.details?.strengths || [])
        const allWeaknesses = cs.allDetails.flatMap(d => d.weaknesses || d?.details?.weaknesses || [])
        const allImprovements = cs.allDetails.flatMap(d => d.improvement || d?.details?.improvement || [])
        const allEvidence = cs.allDetails.map(d => d.evidence || d?.details?.evidence).filter(Boolean)

        // Select unique and representative feedback
        const strengths = [...new Set(allStrengths)].slice(0, 2).join('\n')
        const weaknesses = [...new Set(allWeaknesses)].slice(0, 2).join('\n')
        const improvement = [...new Set(allImprovements)].slice(0, 2).join('\n')

        // Select the longest evidence description
        const evidence = allEvidence.sort((a, b) => b.length - a.length)[0] || ''

        return {
            name: cs.name,
            score: Math.round(cs.scoreSum / cs.count),
            maxScore: cs.maxScore,
            // Reconstruct detailed feedback structure
            evidence,
            strengths,
            weaknesses,
            improvement,
            details: cs.allDetails[0] // Fallback
        }
    })

    // Merge characteristics (unique values)
    const allCharacteristics = results.flatMap(r => r.characteristics || [])
    const characteristics = [...new Set(allCharacteristics)].slice(0, 5)

    // Use first qualitative feedback (they should be similar)
    const qualitative = results[0]?.qualitative || []

    // Merge suggestions (unique values)
    const allSuggestions = results.flatMap(r => r.suggestions || [])
    const suggestions = [...new Set(allSuggestions)].slice(0, 4)

    // Pick the longest student record draft
    const studentRecordDraft = results
        .map(r => r.studentRecordDraft || '')
        .sort((a, b) => b.length - a.length)[0] || ''

    return {
        totalScore: avgScore,
        grade,
        criteriaScores,
        characteristics,
        qualitative,
        suggestions,
        studentRecordDraft,
        evaluationMeta: {
            runs: n,
            scoreRange: { min: minScore, max: maxScore },
            variance: maxScore - minScore
        }
    }
}

/**
 * Calculate grade from score
 */
function calculateGrade(score) {
    if (score >= 95) return 'A+'
    if (score >= 90) return 'A'
    if (score >= 85) return 'B+'
    if (score >= 80) return 'B'
    if (score >= 75) return 'C+'
    if (score >= 70) return 'C'
    if (score >= 65) return 'D+'
    if (score >= 60) return 'D'
    return 'F'
}

/**
 * Server Proxy í˜¸ì¶œ (/api/evaluate)
 */
async function callServerProxy({ prompt, provider, model }) {
    const response = await fetch('/api/evaluate', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            prompt,
            provider,
            model
        })
    })

    if (!response.ok) {
        const error = await response.json().catch(() => ({}))
        throw new Error(error.error || `Server Error: ${response.status}`)
    }

    const data = await response.json()
    return data.text || ''
}

/**
 * í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
 */
function buildEvaluationPrompt(chatContent, rubric, reflection) {
    const criteriaDescription = rubric.criteria.map(c => {
        const levelsDesc = c.levels
            .map(l => `  - ${l.score}ì : ${l.description}`)
            .join('\n')
        return `### ${c.name} (ê°€ì¤‘ì¹˜: ${c.weight}%)
ì„¤ëª…: ${c.description}
í‰ê°€ ìˆ˜ì¤€:
${levelsDesc}`
    }).join('\n\n')

    // Generate criteria list for clearer instruction
    const criteriaNamesList = rubric.criteria.map((c, i) => `${i + 1}. ${c.name}`).join('\n')

    return `ë‹¹ì‹ ì€ AI ì±„íŒ… í™œìš© ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í•™ìƒë“¤ì´ AIë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ì´ê³  êµìœ¡ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.

# í‰ê°€ ë£¨ë¸Œë¦­: ${rubric.name}

${criteriaDescription}

# í•™ìƒ ìê¸°í‰ê°€ / ì¶”ê°€ ë§¥ë½ (Additional Context)
${reflection ? reflection : "(ì—†ìŒ)"}

âš ï¸ ì£¼ì˜: ìœ„ 'í•™ìƒ ìê¸°í‰ê°€' ë‚´ìš©ì€ **ì •ì„± í‰ê°€(ì˜ê²¬, ìƒí™œê¸°ë¡ë¶€)**ì—ë§Œ ë°˜ì˜í•˜ê³ , **ì ìˆ˜(Quantitative Score)** ì‚°ì •ì—ëŠ” ì ˆëŒ€ ë°˜ì˜í•˜ì§€ ë§ˆì„¸ìš”. ì ìˆ˜ëŠ” ì˜¤ì§ ì±„íŒ… ë‚´ìš©ì˜ í’ˆì§ˆë¡œë§Œ í‰ê°€í•˜ì„¸ìš”.

# í‰ê°€í•  ì±„íŒ… ê¸°ë¡ (âš ï¸ ì¤‘ìš” ì§€ì¹¨)
ë³¸ ë‚´ìš©ì€ ì‚¬ìš©ìê°€ ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ë³µì‚¬í•˜ì—¬ ë¶™ì—¬ë„£ì€ ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì‚¬ì´ë“œë°”ì˜ ì±„íŒ… ëª©ë¡, ì„¤ì • ë©”ë‰´, ë²„íŠ¼ í…ìŠ¤íŠ¸ ë“± ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**í‰ê°€ ì‹œ ë°˜ë“œì‹œ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:**
1. ì±„íŒ… í™”ë©´ì˜ **ì£¼ìš” ëŒ€í™” ë‚´ìš©(ì‚¬ìš©ì ì§ˆë¬¸ê³¼ AI ì‘ë‹µ)**ë§Œ ì¶”ì¶œí•˜ì—¬ í‰ê°€ì— ë°˜ì˜í•˜ì„¸ìš”.
2. ì‚¬ì´ë“œë°”ì˜ ë‹¤ë¥¸ ì±„íŒ… ì œëª©ì´ë‚˜ ë©”ë‰´ ë²„íŠ¼ ë“± ëŒ€í™” ì™¸ì ì¸ í…ìŠ¤íŠ¸ëŠ” **ì™„ì „í•œ ë¬´ì‹œ(Ignore)** ëŒ€ìƒìœ¼ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”.
3. ë§Œì•½ ë³µì‚¬ëœ ë‚´ìš© ì¤‘ ì—¬ëŸ¬ ëŒ€í™”ê°€ ì„ì—¬ ìˆë‹¤ë©´, ê°€ì¥ ë§ˆì§€ë§‰ì— ì§„í–‰ëœ ì£¼ìš” ì£¼ì œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

---
${chatContent}
---

# í‰ê°€ ê²°ê³¼ í˜•ì‹

âš ï¸ ì¤‘ìš”: ì•„ë˜ ${rubric.criteria.length}ê°œ í‰ê°€ í•­ëª©ì„ **ëª¨ë‘** í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤:
${criteriaNamesList}

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
criteriaScores ë°°ì—´ì—ëŠ” ë°˜ë“œì‹œ **${rubric.criteria.length}ê°œ í•­ëª©**ì´ í¬í•¨ë˜ì–´ì•¼ í•˜ë©°, **ê° í•­ëª©ë§ˆë‹¤ evidence, strengths, weaknesses, improvement í•„ë“œê°€ ë¹„ì–´ìˆì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤**.

\`\`\`json
{
  "totalScore": 85,
  "grade": "B+",
  "criteriaScores": [
    {
      "criterionId": "criterion_1",
      "name": "ì²« ë²ˆì§¸ í‰ê°€ í•­ëª©ëª…",
      "score": 4,
      "maxScore": 5,
      "percentage": 80,
      "evidence": "ì±„íŒ… ë‚´ìš© ì¤‘ í•´ë‹¹ ì ìˆ˜ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë¶€ë¶„ì„ ì§ì ‘ ì¸ìš©í•˜ê±°ë‚˜ ìš”ì•½",
      "strengths": "ì˜í•œ ì ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¹­ì°¬",
      "weaknesses": "ë¶€ì¡±í•œ ì  ì§€ì ",
      "improvement": "êµ¬ì²´ì  ê°œì„  ì˜ˆì‹œ (Before/After)"
    },
    {
      "criterionId": "criterion_2",
      "name": "ë‘ ë²ˆì§¸ í‰ê°€ í•­ëª©ëª…",
      "score": 3,
      "maxScore": 5,
      "percentage": 60,
      "evidence": "...",
      "strengths": "...",
      "weaknesses": "...",
      "improvement": "..."
    }
    // ... ë‚˜ë¨¸ì§€ í‰ê°€ í•­ëª©ë„ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ëª¨ë‘ í¬í•¨
  ],
  "characteristics": [
    "ì´ í•™ìƒì˜ AI í™œìš© íŠ¹ì§• 1",
    "íŠ¹ì§• 2",
    "íŠ¹ì§• 3"
  ],
  "qualitativeEvaluation": "ì „ë°˜ì ì¸ ì •ì„± í‰ê°€. í•™ìƒì˜ ê°•ì ê³¼ ì„±ì¥ ê°€ëŠ¥ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±. (í•™ìƒì˜ ìê¸°í‰ê°€ ë‚´ìš©ë„ ì°¸ê³ í•˜ì—¬ ê²©ë ¤)",
  "suggestions": [
    "êµ¬ì²´ì ì¸ ì‹¤ì²œ ë°©ì•ˆ 1",
    "êµ¬ì²´ì ì¸ ì‹¤ì²œ ë°©ì•ˆ 2"
  ],
  "studentRecordDraft": "ìƒí™œê¸°ë¡ë¶€ ì‘ì„±ìš© ì´ˆì•ˆ (í•™ìƒì˜ ìê¸°í‰ê°€ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì´ë¥¼ í¬í•¨í•˜ì—¬, êµ¬ì²´ì ì¸ í™œë™ ë§¥ë½ì´ ë“œëŸ¬ë‚˜ë„ë¡ 3-4ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±)"
}
\`\`\`

# í•„ìˆ˜ ì§€ì¹¨ (ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”!)

1. **criteriaScoresëŠ” ë°˜ë“œì‹œ ${rubric.criteria.length}ê°œ**ì—¬ì•¼ í•©ë‹ˆë‹¤. í•˜ë‚˜ë¼ë„ ë¹ ì§€ë©´ ì•ˆ ë©ë‹ˆë‹¤.
2. **evidence í•„ë“œ ì‘ì„±ë²•**:
   - ë‹¨ìˆœíˆ "ì˜í–ˆìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì§€ ë§ê³ , **ì±„íŒ…ì˜ íŠ¹ì • ëŒ€ëª©**ì„ ì½• ì§‘ì–´ì„œ ì–¸ê¸‰í•˜ì„¸ìš”.
   - í•™ìƒì´ "ë‚´ íë¦„ë„ë¥¼ ë°˜ì˜í•´ë‹¬ë¼"ê³  í–ˆë‹¤ë©´, ê·¸ ë…¸ë ¥ì„ ì •ì„± í‰ê°€ì— ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
3. **improvement í•„ë“œê°€ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤!** êµ¬ì²´ì ì¸ ìˆ˜ì • ì˜ˆì‹œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”.
4. totalScoreëŠ” ê° í•­ëª© ì ìˆ˜ì— ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ 100ì  ë§Œì  í™˜ì‚° ì ìˆ˜ì…ë‹ˆë‹¤.
5. evidence, strengths, weaknesses, improvement í•„ë“œëŠ” **ë¹ˆ ë¬¸ìì—´("")ì´ë©´ ì•ˆ ë©ë‹ˆë‹¤**. ë°˜ë“œì‹œ ë‚´ìš©ì„ ì±„ì›Œì£¼ì„¸ìš”.
6. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ì£¼ì„ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.`
}

/**
 * Gemini API í˜¸ì¶œ
 */
async function callGeminiAPI(prompt, apiKey, model = 'gemini-2.5-pro') {
    const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`

    const response = await fetch(url, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            contents: [{
                parts: [{ text: prompt }]
            }],
            generationConfig: {
                temperature: 0.3,
                maxOutputTokens: 8192
            }
        })
    })

    if (!response.ok) {
        const error = await response.json().catch(() => ({}))
        throw new Error(error.error?.message || `Gemini API ì˜¤ë¥˜: ${response.status}`)
    }

    const data = await response.json()
    return data.candidates?.[0]?.content?.parts?.[0]?.text || ''
}

/**
 * OpenAI API í˜¸ì¶œ
 */
async function callOpenAIAPI(prompt, apiKey, model = 'gpt-4o') {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`
        },
        body: JSON.stringify({
            model,
            messages: [
                { role: 'system', content: 'You are an educational AI evaluation expert. Always respond in valid JSON format.' },
                { role: 'user', content: prompt }
            ],
            temperature: 0.3,
            max_tokens: 8192
        })
    })

    if (!response.ok) {
        const error = await response.json().catch(() => ({}))
        throw new Error(error.error?.message || `OpenAI API ì˜¤ë¥˜: ${response.status}`)
    }

    const data = await response.json()
    return data.choices?.[0]?.message?.content || ''
}

/**
 * Claude API í˜¸ì¶œ
 */
async function callClaudeAPI(prompt, apiKey, model = 'claude-3-5-sonnet-20241022') {
    const response = await fetch('https://api.anthropic.com/v1/messages', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'x-api-key': apiKey,
            'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify({
            model,
            max_tokens: 8192,
            messages: [
                { role: 'user', content: prompt }
            ]
        })
    })

    if (!response.ok) {
        const error = await response.json().catch(() => ({}))
        throw new Error(error.error?.message || `Claude API ì˜¤ë¥˜: ${response.status}`)
    }

    const data = await response.json()
    return data.content?.[0]?.text || ''
}

/**
 * Client-side Ensemble Execution
 * Only calls services that are enabled in the selection
 */
async function evaluateEnsembleOnClient(prompt, apiKeys, ensembleModels, enabledServices = { gemini: true, openai: true, claude: true }) {
    console.log('Starting Client-Side Ensemble with models:', ensembleModels, 'Enabled:', enabledServices)

    // Use user-selected models or defaults
    const geminiModel = ensembleModels?.gemini || 'gemini-2.5-flash'
    const openaiModel = ensembleModels?.openai || 'gpt-4o'
    const claudeModel = ensembleModels?.claude || 'claude-3-5-sonnet-20241022'

    // Build promises array based on enabled services (default to true if not explicitly set)
    const promises = []
    const serviceNames = []

    const geminiEnabled = enabledServices?.gemini ?? true
    const openaiEnabled = enabledServices?.openai ?? true
    const claudeEnabled = enabledServices?.claude ?? true

    console.log('Service states:', { geminiEnabled, openaiEnabled, claudeEnabled })
    console.log('API Keys present:', {
        gemini: !!apiKeys.gemini,
        openai: !!apiKeys.openai,
        claude: !!apiKeys.claude
    })

    if (geminiEnabled && apiKeys.gemini) {
        promises.push(callGeminiAPI(prompt, apiKeys.gemini, geminiModel))
        serviceNames.push('Gemini')
    }
    if (openaiEnabled && apiKeys.openai) {
        promises.push(callOpenAIAPI(prompt, apiKeys.openai, openaiModel))
        serviceNames.push('OpenAI')
    }
    if (claudeEnabled && apiKeys.claude) {
        promises.push(callClaudeAPI(prompt, apiKeys.claude, claudeModel))
        serviceNames.push('Claude')
    }

    if (promises.length < 2) {
        throw new Error(`ì•™ìƒë¸” í‰ê°€ì—ëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ AI ì„œë¹„ìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. (í˜„ì¬ í™œì„±: ${serviceNames.join(', ') || 'ì—†ìŒ'})`)
    }

    console.log(`Ensemble evaluating with: ${serviceNames.join(', ')}`)

    const results = await Promise.allSettled(promises)

    const successfulResults = results
        .filter(r => r.status === 'fulfilled')
        .map(r => r.value)

    if (successfulResults.length === 0) {
        const errors = results
            .filter(r => r.status === 'rejected')
            .map(r => r.reason?.message || 'Unknown error')
            .join(', ')
        throw new Error(`ëª¨ë“  AI ëª¨ë¸ì´ ì‘ë‹µí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (Errors: ${errors})`)
    }

    return synthesizeResults(successfulResults)
}

/**
 * Synthesize multiple JSON results (Client-side version)
 */
function synthesizeResults(texts) {
    const validResults = []

    // Parse each result
    texts.forEach(text => {
        try {
            const match = text.match(/```(?:json)?\s*([\s\S]*?)```/) || [null, text]
            const jsonStr = match[1].trim()
            const obj = JSON.parse(jsonStr)
            if (obj.totalScore !== undefined) {
                validResults.push(obj)
            }
        } catch (e) {
            console.warn('Failed to parse result in ensemble:', e)
        }
    })

    if (validResults.length === 0) {
        return texts[0] || "{}"
    }

    const count = validResults.length
    const base = validResults[0]

    const finalResult = {
        ...base,
        totalScore: Math.round(validResults.reduce((acc, r) => acc + (r.totalScore || 0), 0) / count),
        qualitativeEvaluation: validResults.map((r, i) => `[ì˜ê²¬ ${i + 1}]\n${r.qualitativeEvaluation}`).join('\n\n---\n\n'),
        characteristics: [...new Set(validResults.flatMap(r => r.characteristics || []))],
        suggestions: [...new Set(validResults.flatMap(r => r.suggestions || []))],
        criteriaScores: base.criteriaScores.map((criterion, idx) => {
            const sum = validResults.reduce((acc, r) => {
                const c = r.criteriaScores[idx]
                return acc + (c ? (c.score || 0) : 0)
            }, 0)
            const avgScore = Math.round(sum / count)

            const combinedStrengths = validResults.map(r => r.criteriaScores[idx]?.strengths).filter(Boolean).join(' / ')
            const combinedWeaknesses = validResults.map(r => r.criteriaScores[idx]?.weaknesses).filter(Boolean).join('\n')
            const combinedImprovement = validResults.map(r => r.criteriaScores[idx]?.improvement).filter(Boolean).join('\n')

            // Generate combined evidence properly
            const combinedEvidence = validResults.map(r => r.criteriaScores[idx]?.evidence).filter(Boolean).join('\n')

            return {
                ...criterion,
                score: avgScore,
                strengths: combinedStrengths,
                weaknesses: combinedWeaknesses,
                improvement: combinedImprovement,
                evidence: combinedEvidence
            }
        })
    }

    return JSON.stringify(finalResult, null, 2)
}

/**
 * í‰ê°€ ì‘ë‹µ íŒŒì‹±
 */
function parseEvaluationResponse(response, rubric) {
    // JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
    let jsonStr = response

    // ```json ... ``` í˜•ì‹ ì²˜ë¦¬
    const jsonMatch = response.match(/```(?:json)?\s*([\s\S]*?)```/)
    if (jsonMatch) {
        jsonStr = jsonMatch[1]
    }

    // ì•ë’¤ ê³µë°± ì œê±°
    jsonStr = jsonStr.trim()

    try {
        if (!jsonStr) throw new Error('Empty JSON string')
        const result = JSON.parse(jsonStr)

        // í•„ìˆ˜ í•„ë“œ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì„¤ì •
        return {
            totalScore: result.totalScore || 0,
            grade: result.grade || 'N/A',
            criteriaScores: (result.criteriaScores || []).map((cs, idx) => {
                // Percentage ìë™ ê³„ì‚° (AIê°€ ëˆ„ë½í•  ê²½ìš° ëŒ€ë¹„)
                const safeScore = cs.score || 0
                const safeMax = cs.maxScore || 5
                const calculatedPercentage = Math.round((safeScore / safeMax) * 100)

                // Check for missing details
                if (!cs.strengths && !cs.weaknesses) {
                    console.warn(`âš ï¸ [DEBUG] Missing details for criterion ${idx + 1}:`, cs)
                }

                return {
                    criterionId: cs.criterionId || '',
                    name: cs.name || '',
                    score: safeScore,
                    maxScore: safeMax,
                    percentage: cs.percentage !== undefined ? cs.percentage : calculatedPercentage,
                    evidence: cs.evidence || cs.feedback || 'ê·¼ê±°ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.', // Fallback to feedback
                    strengths: cs.strengths || '',
                    weaknesses: cs.weaknesses || '',
                    improvement: cs.improvement || 'ì¶”ê°€ì ì¸ ê°œì„  ì œì•ˆì´ ì—†ìŠµë‹ˆë‹¤.',
                    feedback: cs.feedback || '' // ì´ì „ ë²„ì „ í˜¸í™˜
                }
            }),
            characteristics: result.characteristics || [],
            qualitativeEvaluation: result.qualitativeEvaluation || '',
            suggestions: result.suggestions || [],
            studentRecordDraft: result.studentRecordDraft || ''
        }
    } catch (error) {
        console.error('JSON íŒŒì‹± ì˜¤ë¥˜:', error)
        console.log('ì›ë³¸ ì‘ë‹µ:', response)

        // íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ìƒì„±
        return {
            totalScore: 0,
            grade: 'N/A',
            criteriaScores: rubric.criteria.map(c => ({
                criterionId: c.id,
                name: c.name,
                score: 0,
                maxScore: 5,
                percentage: 0,
                feedback: 'í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            })),
            characteristics: ['í‰ê°€ ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜'],
            qualitativeEvaluation: `AI ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\nì›ë³¸ ì‘ë‹µ:\n${response.substring(0, 500)}...`,
            suggestions: ['ë‹¤ì‹œ í‰ê°€ë¥¼ ì‹œë„í•´ ì£¼ì„¸ìš”.'],
            studentRecordDraft: ''
        }
    }
}
